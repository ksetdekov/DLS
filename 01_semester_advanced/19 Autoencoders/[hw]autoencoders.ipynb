{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ7i1HkmYY68"
      },
      "source": [
        "<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n",
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Домашнее задание. Весна 2021</b></h3>\n",
        "\n",
        "# Autoencoders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wru2LNFuL2Iq"
      },
      "source": [
        "# Часть 1. Vanilla Autoencoder (10 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3STtdpYY7G"
      },
      "source": [
        "## 1.1. Подготовка данных (0.5 балла)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xTNi9JLRYY7I",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import skimage.io\n",
        "from skimage.transform import resize\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zvAjov5F2NvE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n",
        "                      images_name = \"lfw-deepfunneled\",\n",
        "                      dx=80,dy=80,\n",
        "                      dimx=64,dimy=64\n",
        "    ):\n",
        "\n",
        "    #download if not exists\n",
        "    if not os.path.exists(images_name):\n",
        "        print(\"images not found, donwloading...\")\n",
        "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
        "        print(\"extracting...\")\n",
        "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
        "        print(\"done\")\n",
        "        assert os.path.exists(images_name)\n",
        "\n",
        "    if not os.path.exists(attrs_name):\n",
        "        print(\"attributes not found, downloading...\")\n",
        "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
        "        print(\"done\")\n",
        "\n",
        "    #read attrs\n",
        "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
        "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
        "\n",
        "\n",
        "    #read photos\n",
        "    photo_ids = []\n",
        "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
        "        for fname in filenames:\n",
        "            if fname.endswith(\".jpg\"):\n",
        "                fpath = os.path.join(dirpath,fname)\n",
        "                photo_id = fname[:-4].replace('_',' ').split()\n",
        "                person_id = ' '.join(photo_id[:-1])\n",
        "                photo_number = int(photo_id[-1])\n",
        "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
        "\n",
        "    photo_ids = pd.DataFrame(photo_ids)\n",
        "    # print(photo_ids)\n",
        "    #mass-merge\n",
        "    #(photos now have same order as attributes)\n",
        "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
        "\n",
        "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
        "\n",
        "    # print(df.shape)\n",
        "    #image preprocessing\n",
        "    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n",
        "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
        "                                .apply(lambda img: resize(img,[dimx,dimy]))\n",
        "\n",
        "    all_photos = np.stack(all_photos.values)#.astype('uint8')\n",
        "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
        "    \n",
        "    return all_photos, all_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3KhlblLYY7P",
        "outputId": "4d35a263-f2b5-4e62-90c5-8820ba988052",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images not found, donwloading...\n",
            "extracting...\n",
            "done\n",
            "attributes not found, downloading...\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n",
        "# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
        "\n",
        "data, attrs = fetch_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MSzXXGoYY7X"
      },
      "source": [
        "\n",
        "Разбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFc8lTm_YY7Y",
        "outputId": "ca3c5e8f-4fd7-4bc4-cb3d-e33d8d58c338",
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of data(13143, 64, 64, 3)\n",
            "shape of attr(13143, 73)\n"
          ]
        }
      ],
      "source": [
        "print(f\"shape of data{data.shape}\")\n",
        "print(f\"shape of attr{attrs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "skkoxypXPGIt",
        "outputId": "89a45a49-b0a2-4006-dd05-b1df0c2194a2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d5e4fd17-7204-4a0a-8070-7abe98ae78e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Male</th>\n",
              "      <th>Asian</th>\n",
              "      <th>White</th>\n",
              "      <th>Black</th>\n",
              "      <th>Baby</th>\n",
              "      <th>Child</th>\n",
              "      <th>Youth</th>\n",
              "      <th>Middle Aged</th>\n",
              "      <th>Senior</th>\n",
              "      <th>Black Hair</th>\n",
              "      <th>...</th>\n",
              "      <th>Pale Skin</th>\n",
              "      <th>5 o' Clock Shadow</th>\n",
              "      <th>Strong Nose-Mouth Lines</th>\n",
              "      <th>Wearing Lipstick</th>\n",
              "      <th>Flushed Face</th>\n",
              "      <th>High Cheekbones</th>\n",
              "      <th>Brown Eyes</th>\n",
              "      <th>Wearing Earrings</th>\n",
              "      <th>Wearing Necktie</th>\n",
              "      <th>Wearing Necklace</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.568346</td>\n",
              "      <td>-1.889043</td>\n",
              "      <td>1.737203</td>\n",
              "      <td>-0.929729</td>\n",
              "      <td>-1.471799</td>\n",
              "      <td>-0.19558</td>\n",
              "      <td>-0.835609</td>\n",
              "      <td>-0.351468</td>\n",
              "      <td>-1.012533</td>\n",
              "      <td>-0.719593</td>\n",
              "      <td>...</td>\n",
              "      <td>0.361738</td>\n",
              "      <td>1.166118</td>\n",
              "      <td>-1.164916</td>\n",
              "      <td>-1.13999</td>\n",
              "      <td>-2.371746</td>\n",
              "      <td>-1.299932</td>\n",
              "      <td>-0.414682</td>\n",
              "      <td>-1.144902</td>\n",
              "      <td>0.694007</td>\n",
              "      <td>-0.826609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.169851</td>\n",
              "      <td>-0.982408</td>\n",
              "      <td>0.422709</td>\n",
              "      <td>-1.282184</td>\n",
              "      <td>-1.36006</td>\n",
              "      <td>-0.867002</td>\n",
              "      <td>-0.452293</td>\n",
              "      <td>-0.197521</td>\n",
              "      <td>-0.956073</td>\n",
              "      <td>-0.802107</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.832036</td>\n",
              "      <td>-0.39768</td>\n",
              "      <td>0.87416</td>\n",
              "      <td>-0.945431</td>\n",
              "      <td>-0.268649</td>\n",
              "      <td>-0.006244</td>\n",
              "      <td>-0.030406</td>\n",
              "      <td>-0.480128</td>\n",
              "      <td>0.66676</td>\n",
              "      <td>-0.496559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.997749</td>\n",
              "      <td>-1.364195</td>\n",
              "      <td>-0.157377</td>\n",
              "      <td>-0.756447</td>\n",
              "      <td>-1.891825</td>\n",
              "      <td>-0.871526</td>\n",
              "      <td>-0.862893</td>\n",
              "      <td>0.031445</td>\n",
              "      <td>-1.341523</td>\n",
              "      <td>-0.090037</td>\n",
              "      <td>...</td>\n",
              "      <td>1.549743</td>\n",
              "      <td>1.884745</td>\n",
              "      <td>-0.999765</td>\n",
              "      <td>-1.359858</td>\n",
              "      <td>-1.912108</td>\n",
              "      <td>-1.095634</td>\n",
              "      <td>0.915126</td>\n",
              "      <td>-0.572332</td>\n",
              "      <td>0.144262</td>\n",
              "      <td>-0.841231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.122719</td>\n",
              "      <td>-1.997799</td>\n",
              "      <td>1.916144</td>\n",
              "      <td>-2.514214</td>\n",
              "      <td>-2.580071</td>\n",
              "      <td>-1.404239</td>\n",
              "      <td>0.057551</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>-1.273512</td>\n",
              "      <td>-1.431462</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567822</td>\n",
              "      <td>-0.176089</td>\n",
              "      <td>1.108125</td>\n",
              "      <td>-1.600944</td>\n",
              "      <td>-3.264613</td>\n",
              "      <td>0.813418</td>\n",
              "      <td>0.308631</td>\n",
              "      <td>-0.848693</td>\n",
              "      <td>0.475941</td>\n",
              "      <td>-0.447025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.078214</td>\n",
              "      <td>-2.008098</td>\n",
              "      <td>1.676211</td>\n",
              "      <td>-2.278056</td>\n",
              "      <td>-2.651845</td>\n",
              "      <td>-1.348408</td>\n",
              "      <td>0.649089</td>\n",
              "      <td>0.017656</td>\n",
              "      <td>-1.889111</td>\n",
              "      <td>-1.857213</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.461474</td>\n",
              "      <td>-0.955283</td>\n",
              "      <td>0.119113</td>\n",
              "      <td>-1.128176</td>\n",
              "      <td>-3.161048</td>\n",
              "      <td>0.08268</td>\n",
              "      <td>-0.439614</td>\n",
              "      <td>-0.359859</td>\n",
              "      <td>-0.760774</td>\n",
              "      <td>-0.410152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13138</th>\n",
              "      <td>-0.205363</td>\n",
              "      <td>-0.202842</td>\n",
              "      <td>-1.232735</td>\n",
              "      <td>-1.409125</td>\n",
              "      <td>-1.804858</td>\n",
              "      <td>0.287268</td>\n",
              "      <td>-0.521815</td>\n",
              "      <td>-0.789165</td>\n",
              "      <td>-1.008119</td>\n",
              "      <td>0.531813</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.867156</td>\n",
              "      <td>-0.879554</td>\n",
              "      <td>-0.179497</td>\n",
              "      <td>0.323676</td>\n",
              "      <td>-2.188852</td>\n",
              "      <td>-1.169369</td>\n",
              "      <td>0.924397</td>\n",
              "      <td>-0.217415</td>\n",
              "      <td>-0.414024</td>\n",
              "      <td>-0.495178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13139</th>\n",
              "      <td>1.957472</td>\n",
              "      <td>-2.47247</td>\n",
              "      <td>-1.307994</td>\n",
              "      <td>-0.671636</td>\n",
              "      <td>-1.960125</td>\n",
              "      <td>-0.838803</td>\n",
              "      <td>-1.365955</td>\n",
              "      <td>0.510448</td>\n",
              "      <td>-1.262193</td>\n",
              "      <td>0.418947</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.487289</td>\n",
              "      <td>2.608861</td>\n",
              "      <td>-0.95419</td>\n",
              "      <td>-1.599463</td>\n",
              "      <td>-1.23822</td>\n",
              "      <td>-1.187034</td>\n",
              "      <td>1.87413</td>\n",
              "      <td>-0.832614</td>\n",
              "      <td>0.152785</td>\n",
              "      <td>-0.554331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13140</th>\n",
              "      <td>-0.037001</td>\n",
              "      <td>-1.16415</td>\n",
              "      <td>0.228494</td>\n",
              "      <td>0.187503</td>\n",
              "      <td>-1.836475</td>\n",
              "      <td>-0.958412</td>\n",
              "      <td>-0.669697</td>\n",
              "      <td>-0.48033</td>\n",
              "      <td>0.493433</td>\n",
              "      <td>-1.893151</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.938639</td>\n",
              "      <td>-0.49108</td>\n",
              "      <td>0.495067</td>\n",
              "      <td>-0.524767</td>\n",
              "      <td>-1.073731</td>\n",
              "      <td>0.11101</td>\n",
              "      <td>0.976262</td>\n",
              "      <td>-0.453027</td>\n",
              "      <td>-0.399981</td>\n",
              "      <td>-1.078655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13141</th>\n",
              "      <td>0.282219</td>\n",
              "      <td>-0.396198</td>\n",
              "      <td>1.27047</td>\n",
              "      <td>-1.981209</td>\n",
              "      <td>-1.70294</td>\n",
              "      <td>-1.507203</td>\n",
              "      <td>-2.330721</td>\n",
              "      <td>-0.295928</td>\n",
              "      <td>1.056618</td>\n",
              "      <td>-1.613698</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.662101</td>\n",
              "      <td>-1.692131</td>\n",
              "      <td>1.284772</td>\n",
              "      <td>-0.80818</td>\n",
              "      <td>-0.710051</td>\n",
              "      <td>0.694621</td>\n",
              "      <td>0.936065</td>\n",
              "      <td>-0.7675</td>\n",
              "      <td>1.14693</td>\n",
              "      <td>-0.26094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13142</th>\n",
              "      <td>0.071197</td>\n",
              "      <td>-1.92884</td>\n",
              "      <td>2.122345</td>\n",
              "      <td>-1.892862</td>\n",
              "      <td>-2.044238</td>\n",
              "      <td>-0.313663</td>\n",
              "      <td>-0.081369</td>\n",
              "      <td>-0.140593</td>\n",
              "      <td>-1.277778</td>\n",
              "      <td>-0.982043</td>\n",
              "      <td>...</td>\n",
              "      <td>0.773205</td>\n",
              "      <td>-0.404315</td>\n",
              "      <td>-1.100768</td>\n",
              "      <td>-0.872605</td>\n",
              "      <td>-3.490632</td>\n",
              "      <td>-0.840752</td>\n",
              "      <td>0.532488</td>\n",
              "      <td>-0.946166</td>\n",
              "      <td>-0.232008</td>\n",
              "      <td>-1.257209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13143 rows × 73 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5e4fd17-7204-4a0a-8070-7abe98ae78e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5e4fd17-7204-4a0a-8070-7abe98ae78e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5e4fd17-7204-4a0a-8070-7abe98ae78e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Male     Asian     White     Black      Baby     Child     Youth  \\\n",
              "0      1.568346 -1.889043  1.737203 -0.929729 -1.471799  -0.19558 -0.835609   \n",
              "1      0.169851 -0.982408  0.422709 -1.282184  -1.36006 -0.867002 -0.452293   \n",
              "2      0.997749 -1.364195 -0.157377 -0.756447 -1.891825 -0.871526 -0.862893   \n",
              "3      1.122719 -1.997799  1.916144 -2.514214 -2.580071 -1.404239  0.057551   \n",
              "4      1.078214 -2.008098  1.676211 -2.278056 -2.651845 -1.348408  0.649089   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "13138 -0.205363 -0.202842 -1.232735 -1.409125 -1.804858  0.287268 -0.521815   \n",
              "13139  1.957472  -2.47247 -1.307994 -0.671636 -1.960125 -0.838803 -1.365955   \n",
              "13140 -0.037001  -1.16415  0.228494  0.187503 -1.836475 -0.958412 -0.669697   \n",
              "13141  0.282219 -0.396198   1.27047 -1.981209  -1.70294 -1.507203 -2.330721   \n",
              "13142  0.071197  -1.92884  2.122345 -1.892862 -2.044238 -0.313663 -0.081369   \n",
              "\n",
              "      Middle Aged    Senior Black Hair  ... Pale Skin 5 o' Clock Shadow  \\\n",
              "0       -0.351468 -1.012533  -0.719593  ...  0.361738          1.166118   \n",
              "1       -0.197521 -0.956073  -0.802107  ... -0.832036          -0.39768   \n",
              "2        0.031445 -1.341523  -0.090037  ...  1.549743          1.884745   \n",
              "3        0.000196 -1.273512  -1.431462  ...  0.567822         -0.176089   \n",
              "4        0.017656 -1.889111  -1.857213  ... -1.461474         -0.955283   \n",
              "...           ...       ...        ...  ...       ...               ...   \n",
              "13138   -0.789165 -1.008119   0.531813  ... -0.867156         -0.879554   \n",
              "13139    0.510448 -1.262193   0.418947  ... -2.487289          2.608861   \n",
              "13140    -0.48033  0.493433  -1.893151  ... -1.938639          -0.49108   \n",
              "13141   -0.295928  1.056618  -1.613698  ... -0.662101         -1.692131   \n",
              "13142   -0.140593 -1.277778  -0.982043  ...  0.773205         -0.404315   \n",
              "\n",
              "      Strong Nose-Mouth Lines Wearing Lipstick Flushed Face High Cheekbones  \\\n",
              "0                   -1.164916         -1.13999    -2.371746       -1.299932   \n",
              "1                     0.87416        -0.945431    -0.268649       -0.006244   \n",
              "2                   -0.999765        -1.359858    -1.912108       -1.095634   \n",
              "3                    1.108125        -1.600944    -3.264613        0.813418   \n",
              "4                    0.119113        -1.128176    -3.161048         0.08268   \n",
              "...                       ...              ...          ...             ...   \n",
              "13138               -0.179497         0.323676    -2.188852       -1.169369   \n",
              "13139                -0.95419        -1.599463     -1.23822       -1.187034   \n",
              "13140                0.495067        -0.524767    -1.073731         0.11101   \n",
              "13141                1.284772         -0.80818    -0.710051        0.694621   \n",
              "13142               -1.100768        -0.872605    -3.490632       -0.840752   \n",
              "\n",
              "      Brown Eyes Wearing Earrings Wearing Necktie Wearing Necklace  \n",
              "0      -0.414682        -1.144902        0.694007        -0.826609  \n",
              "1      -0.030406        -0.480128         0.66676        -0.496559  \n",
              "2       0.915126        -0.572332        0.144262        -0.841231  \n",
              "3       0.308631        -0.848693        0.475941        -0.447025  \n",
              "4      -0.439614        -0.359859       -0.760774        -0.410152  \n",
              "...          ...              ...             ...              ...  \n",
              "13138   0.924397        -0.217415       -0.414024        -0.495178  \n",
              "13139    1.87413        -0.832614        0.152785        -0.554331  \n",
              "13140   0.976262        -0.453027       -0.399981        -1.078655  \n",
              "13141   0.936065          -0.7675         1.14693         -0.26094  \n",
              "13142   0.532488        -0.946166       -0.232008        -1.257209  \n",
              "\n",
              "[13143 rows x 73 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Total number of columns (73) exceeds max_columns (20) limiting to first (20) columns.\n",
            "Warning: Total number of columns (73) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ],
      "source": [
        "attrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9CC-DUhYY7i"
      },
      "source": [
        "## 1.2. Архитектура модели (1.5 балла)\n",
        "В этом разделе мы напишем и обучем обычный автоэнкодер.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n",
        "\n",
        "\n",
        "^ напомню, что автоэнкодер выглядит вот так"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "csrNCYh-YY7j",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "DIM_CODE = 150 # выберите размер латентного вектора\n",
        "# разные режимы датасета \n",
        "DATA_MODES = ['train', 'val', 'test']\n",
        "# все изображения будут масштабированы к размеру 224x224 px\n",
        "# работаем на видеокарте\n",
        "DEVICE = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCqxy-XQRkAD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет с картинками, который паралельно подгружает их из папок\n",
        "    производит скалирование и превращение в торчевые тензоры\n",
        "    \"\"\"\n",
        "    def __init__(self, files, mode):\n",
        "        super().__init__()\n",
        "        # список файлов для загрузки\n",
        "        self.files = sorted(files)\n",
        "        # режим работы\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "     \n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            self.labels = [path.parent.name for path in self.files]\n",
        "            self.label_encoder.fit(self.labels)\n",
        "\n",
        "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
        "                  pickle.dump(self.label_encoder, le_dump_file)\n",
        "                      \n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "      \n",
        "    def load_sample(self, file):\n",
        "        image = Image.open(file)\n",
        "        image.load()\n",
        "        return image\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
        "        if self.mode == 'test':\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize(299),\n",
        "                transforms.CenterCrop(299), \n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                ])\n",
        "            x = self.load_sample(self.files[index])\n",
        "            x = self._prepare_sample(x)\n",
        "            x = np.array(x / 255, dtype='float32')\n",
        "            x = transform(x)\n",
        "            return x\n",
        "        else:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomResizedCrop(299),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                ])\n",
        "            x = self.load_sample(self.files[index])\n",
        "            x = self._prepare_sample(x)\n",
        "            x = np.array(x / 255, dtype='float32')\n",
        "            x = transform(x)\n",
        "            label = self.labels[index]\n",
        "            label_id = self.label_encoder.transform([label])\n",
        "            y = label_id.item()\n",
        "            return x, y\n",
        "    def _prepare_sample(self, image):\n",
        "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
        "        return np.array(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjr-N8AWee-k"
      },
      "source": [
        "Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SjHNX-rYY7k",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        <определите архитектуры encoder и decoder>\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        <реализуйте forward проход автоэнкодера\n",
        "        в качестве ваозвращаемых переменных -- латентное представление картинки (latent_code) \n",
        "        и полученная реконструкция изображения (reconstruction)>\n",
        "        \n",
        "        return reconstruction, latent_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73lg3bI2YY7m",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "criterion = <loss>\n",
        "\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "optimizer = <Ваш любимый оптимизатор>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpntmZCe5L6i"
      },
      "source": [
        "## 1.3 Обучение (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdxg_3WJYY7o"
      },
      "source": [
        "Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n",
        "\n",
        "А, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H3DOojrYY7o",
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<тут Ваш код тренировки автоэнкодера>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAztAMA4YY7q"
      },
      "source": [
        "Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1J__yvxYY7r",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OPh9O6UYY7s"
      },
      "source": [
        "Not bad, right? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFi96giuYY7t"
      },
      "source": [
        "## 1.4. Sampling (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOtUaPNYYY7t"
      },
      "source": [
        "Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n",
        "\n",
        "Давайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n",
        "\n",
        "__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IZykARRYY7u",
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# сгенерируем 25 рандомных векторов размера latent_space\n",
        "z = np.random.randn(25, <latent_space_dim>)\n",
        "output = <скормите z декодеру>\n",
        "<выведите тут полученные картинки>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ey8dD9s0YY7w"
      },
      "source": [
        "## Time to make fun! (4 балла)\n",
        "\n",
        "Давайте научимся пририсовывать людям улыбки =)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1v-8WwuYY7w"
      },
      "source": [
        "<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGE0M2GDYY7x"
      },
      "source": [
        "План такой:\n",
        "\n",
        "1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n",
        "\n",
        "Найти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n",
        "\n",
        "2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n",
        "\n",
        "3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n",
        "\n",
        "4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1oBX9EeYY7x",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<ваш код здесь>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXI6jprOYY7z"
      },
      "source": [
        "Вуаля! Вы восхитительны!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2UAf0bpYY70"
      },
      "source": [
        "Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnEGmknYY71"
      },
      "source": [
        "# Часть 2: Variational Autoencoder (10 баллов) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWQNRjJq2uTz"
      },
      "source": [
        "Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBXXr9njByYC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rHphW5l8Wgi"
      },
      "source": [
        "## 2.1 Архитектура модели и обучение (2 балла)\n",
        "\n",
        "Реализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoNVT5tYYY74",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        <определите архитектуры encoder и decoder\n",
        "        помните, у encoder должны быть два \"хвоста\", \n",
        "        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n",
        "\n",
        "    def encode(self, x):\n",
        "        <реализуйте forward проход энкодера\n",
        "        в качестве ваозвращаемых переменных -- mu и logsigma>\n",
        "        \n",
        "        return mu, logsigma\n",
        "    \n",
        "    def gaussian_sampler(self, mu, logsigma):\n",
        "        if self.training:\n",
        "            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n",
        "        else:\n",
        "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
        "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
        "            return mu\n",
        "    \n",
        "    def decode(self, z):\n",
        "        <реализуйте forward проход декодера\n",
        "        в качестве возвращаемой переменной -- reconstruction>\n",
        "        \n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x):\n",
        "        <используя encode и decode, реализуйте forward проход автоэнкодера\n",
        "        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n",
        "        return mu, logsigma, reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAB77d-PYY76"
      },
      "source": [
        "Определим лосс и его компоненты для VAE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxJrkXGQo5bp"
      },
      "source": [
        "Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n",
        "\n",
        "Общий лосс будет выглядеть так:\n",
        "\n",
        "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
        "\n",
        "Формула для KL-дивергенции:\n",
        "\n",
        "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
        "\n",
        "В качестве log-likelihood возьмем привычную нам кросс-энтропию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac5ey7uIYY77",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def KL_divergence(mu, logsigma):\n",
        "    \"\"\"\n",
        "    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n",
        "    \"\"\"\n",
        "    loss = <напишите код для KL-дивергенции, пользуясь формулой выше>\n",
        "    return \n",
        "\n",
        "def log_likelihood(x, reconstruction):\n",
        "    \"\"\"\n",
        "    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n",
        "    \"\"\"\n",
        "    loss = <binary cross-entropy>\n",
        "    return loss(reconstruction, x)\n",
        "\n",
        "def loss_vae(x, mu, logsigma, reconstruction):\n",
        "    return <соедините тут две компоненты лосса. Mind the sign!>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPJQu70eYY79"
      },
      "source": [
        "И обучим модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtCjfqXdYY79",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "criterion = loss_vae\n",
        "\n",
        "autoencoder = VAE()\n",
        "\n",
        "optimizer = <Ваш любимый оптимизатор>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY1khca6YY7_",
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<обучите модель на датасете MNIST>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxW_8fkYY8B",
        "scrolled": true
      },
      "source": [
        "Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jd3BWM_YY8C",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQXYIXjoYY8F"
      },
      "source": [
        "Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhhH-osYY8G",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n",
        "z = np.array([np.random.normal(0, 1, 100) for i in range(10)])\n",
        "output = <скормите z декодеру>\n",
        "<выведите тут полученные картинки>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzt-ENxCr6ul"
      },
      "source": [
        "## 2.2. Latent Representation (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIWy670xr-Uv"
      },
      "source": [
        "Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\n",
        "Ваша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n",
        "\n",
        "Это позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n",
        "\n",
        "Плюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n",
        "\n",
        "Подсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n",
        "\n",
        "\n",
        "Итак, план:\n",
        "1. Получить латентные представления картинок тестового датасета\n",
        "2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n",
        "3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk94C6mCsx9c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<ваш код получения латентных представлений, применения TSNE и визуализации>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifxhsvPss5h_"
      },
      "source": [
        "Что вы думаете о виде латентного представления?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESPBHrL3YY8H"
      },
      "source": [
        "__Congrats v2.0!__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIYuKFwijN2U"
      },
      "source": [
        "## 2.3. Conditional VAE (6 баллов)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5l8Bu1RPjUx"
      },
      "source": [
        "Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \n",
        "Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \n",
        "И вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n",
        "\n",
        "Хотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n",
        "\n",
        "И в этой части задания мы научимся такие обучать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8zNIwKPY-6"
      },
      "source": [
        "### Архитектура\n",
        "\n",
        "На картинке ниже представлена архитектура простого Conditional VAE.\n",
        "\n",
        "По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n",
        "\n",
        "То есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6YloFEAPeM4"
      },
      "source": [
        "\n",
        "![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n",
        "\n",
        "![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxg2tDSfRbLF"
      },
      "source": [
        "На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpFbSXLaPrm1"
      },
      "source": [
        "Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0zxklMPwI2"
      },
      "source": [
        "P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar701cHOkDKS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class CVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        <определите архитектуры encoder и decoder\n",
        "        помните, у encoder должны быть два \"хвоста\", \n",
        "        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n",
        "\n",
        "    def encode(self, x, class_num):\n",
        "        <реализуйте forward проход энкодера\n",
        "        в качестве ваозвращаемых переменных -- mu, logsigma и класс картинки>\n",
        "        \n",
        "        return mu, logsigma, class_num\n",
        "    \n",
        "    def gaussian_sampler(self, mu, logsigma):\n",
        "        if self.training:\n",
        "            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n",
        "        else:\n",
        "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
        "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
        "            return mu\n",
        "    \n",
        "    def decode(self, z, class_num):\n",
        "        <реализуйте forward проход декодера\n",
        "        в качестве возвращаемой переменной -- reconstruction>\n",
        "        \n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x):\n",
        "        <используя encode и decode, реализуйте forward проход автоэнкодера\n",
        "        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n",
        "        return mu, logsigma, reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMw-IFyP5A2"
      },
      "source": [
        "### Sampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe1zWyZHkLV2"
      },
      "source": [
        "Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\n",
        "Для MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0SQIhvNP9Dr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<тут нужно научиться сэмплировать из декодера цифры определенного класса>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAWBu8rzQBgQ"
      },
      "source": [
        "Splendid! Вы великолепны!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2S77cm3O1v"
      },
      "source": [
        "### Latent Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7x8Ek_rHTE"
      },
      "source": [
        "Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n",
        "\n",
        "Опять же, нужно покрасить точки в разные цвета в зависимости от класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSCYK7sH3KEc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<ваш код получения латентных представлений, применения TSNE и визуализации>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET8IELWu3Z2c"
      },
      "source": [
        "Что вы думаете насчет этой картинки? Отличается от картинки для VAE?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWkqHjvTCD_8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN3D_k5W_WZz"
      },
      "source": [
        "# BONUS 1: Denoising\n",
        "\n",
        "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a1jkpkCsIU"
      },
      "source": [
        "У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8EN-8jlCtmd"
      },
      "source": [
        "Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \n",
        "То есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1OJg6jhlaZl"
      },
      "source": [
        "<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysI0BCuRDbvm"
      },
      "source": [
        "Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n",
        "\n",
        "В питоне шум можно добавить так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5e746iVDgSm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "noise_factor = 0.5\n",
        "X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fSPkXMtDpd5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B03NQ_sKDvg2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NDiCPYLm2bY"
      },
      "source": [
        "# BONUS 2: Image Retrieval\n",
        "\n",
        "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xao_27WMm7AL"
      },
      "source": [
        "Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y__bdS23ndeY"
      },
      "source": [
        "План:\n",
        "\n",
        "1. Получаем латентные представления всех лиц тренировочного датасета\n",
        "2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n",
        "3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n",
        "4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n",
        "5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IksC2ucIoND-"
      },
      "source": [
        "Немного кода вам в помощь: (feel free to delete everything and write your own)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK0YpLMRoEa0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "codes = <поучите латентные представления картинок из трейна>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KisDrgZdoWdt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# обучаем LSHForest\n",
        "from sklearn.neighbors import LSHForest\n",
        "lshf = LSHForest(n_estimators=50).fit(codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_S5zPb5obam",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_similar(image, n_neighbors=5):\n",
        "  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n",
        "  # прогоняет векторы через декодер и получает картинки ближайших людей\n",
        "\n",
        "  code = <получение латентного представления image>\n",
        "    \n",
        "  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n",
        "\n",
        "  return distances, X_train[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t2kjV5wupLP_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "def show_similar(image):\n",
        "\n",
        "  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n",
        "    \n",
        "    distances,neighbors = get_similar(image,n_neighbors=11)\n",
        "    \n",
        "    plt.figure(figsize=[8,6])\n",
        "    plt.subplot(3,4,1)\n",
        "    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n",
        "    plt.title(\"Original image\")\n",
        "    \n",
        "    for i in range(11):\n",
        "        plt.subplot(3,4,i+2)\n",
        "        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n",
        "        plt.title(\"Dist=%.3f\"%distances[i])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3Ja1UNf_oJq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dFi96giuYY7t",
        "Ey8dD9s0YY7w",
        "KN3D_k5W_WZz",
        "-NDiCPYLm2bY"
      ],
      "name": "[hw]autoencoders.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
